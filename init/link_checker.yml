# Enter the url of your website where the crawling will begin.
website_url: http://www.intheloftstudios.com

# An array of strings, which if found in a link, the link will not be followed.
# When you have certain links that you know to be valid, you may want to add
# them here to reduce the amount of time it takes to crawl your site.
#nofollow:
#  - facebook.com/dialog/share
#  - pinterest.com/pin/create
#  - twitter.com/share

# The tags and attributes that are considered links for checking, split into the
# following levels:
#
# 0: clickable links
# 1: clickable links, media, iframes, meta refreshes
# 2: clickable links, media, iframes, meta refreshes, stylesheets, scripts, forms
# 3: clickable links, media, iframes, meta refreshes, stylesheets, scripts, forms, metadata
filter_level: 2

# Enter a directory to output reports; reports will be grouped by directory by
# domain name.  This is relative to your project root (../bin).
crawl_file_directory: ./reports

# A prefix for crawl files.
crawl_file_prefix: links-report--

# Once you've crawled the site and have a master file, you can generate
# reports based on certain criteria.  For each entry in the array below,
# you should create an array with the keys: suffix, and match.  The value of
# suffix will be appended to the filename of the master file when the
# report is saved.  Use `match` for a string that must be contained by a
# line for it to appear in the report.  Another way to say it is,
# reports will return only the lines that contain the strings entered below.
reports:
  - file_suffix: 40x
    match: HTTP_40
  - file_suffix: status-ok
    match: "├───OK───"
  - file_suffix: broken
    match: "├─BROKEN─"
